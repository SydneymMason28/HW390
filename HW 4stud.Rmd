---
title: "HW 4"
author: "Sydney Mason"
date: "10/29/2024"
output: 
  html_document:
    number_sections: true
---

This homework is designed to give you practice working with statistical/philosophical measures of fairness. 

#
The paper linked below^[https://link.springer.com/article/10.1007/s00146-023-01676-3] discusses potential algorithmic bias in the context of credit.  In particular, banks are now regularly using machine learning algorithms to do an initial screening for credit-worthy loan applicants.  In section 4.5.2, this paper reports the rates at which various racial groups were granted a mortgage.  If we assume that it is a classifier making these predictions^[It is unclear whether this is an algorithm producing these predictions or human] what additional information would be necessary to assess this classifier according to equalized odds?

We would need to see what the underlying economic factors of the persons applying for mortgage and see if the lower approval rates/ higher percentages on mortgages coincide with a lower ability to pay the mortgage rather than just being the produce of discrimination.  This would examine the ground truth portion behind equalized odds, because currently the information examined in 4.5.2 only addresses disparate impact.  With this, it's important to note that often a lower ability to pay or other qualities that would make a bank less likely to lend or add higher fees may be present in these groups, but oftentimes that is because of larger systemic issues that keep the wealth of non-white groups down.  Therefore, although it is likely there is discrimination in the lending process, the more insidious discrimination is farther back and cannot be discovered through equalized odds solely in connection with mortgage rates.

#
Show or argue that the impossibility result discussed in class does not hold when our two fringe cases^[a) perfect predicting classifier and b) perfectly equal proportions of ground truth class labels across the protected variable] are met.  

Firstly, if there is a perfectly predicting classifier, equalized odds will be met because there will be no cases of false positives or false negatives, meaning that the cases of false positives and negatives will be equal and there are in fact equal odds of 0 of getting a false positive or negative.  Secondly, if there are perfectly equal proportions of a ground truth class label across the protected variable, there can be no opportunity for disparate impact as the ground truth is 50-50, meaning that there is in fact no discrimination.  Finally, for sufficiency to be satisfied, since the values are evenly distributed based upon the perfectly predicting classifier and equal proportions, sufficiency is also met since S is independent of Y.

#

How would Rawls's Veil of Ignorance define a protected class?  Further, imagine that we preprocessed data by removing this protected variable from consideration before training out algorithm.  How could this variable make its way into our interpretation of results nonetheless?

Rawls' Veil of Ignorance would define a protected class as one where the needs of the class in question are not being met and as such a baseline by walking a mile in their shoes should be established that will elevate the rest of the classes in question as well.  
The variable could make its way into the interpretation of results in other manners through proxy variables such as in COMPAS.  Although race was not considered as a variable, the zipcodes of arrests were.  The zipcodes, due to redlining, often correlated with racial background and therefore racial components filtered their way into the results through the proxy variables that were included despite the fact that it wasn't originally included.  If data is rich enough, even if one component isn't specifically addressed, there are enough societal markers that can be indicative of someone's belonging to a protected variable.

#

Based on all arguments discussed in class, is the use of COMPAS to supplement a judge's discretion justifiable.  Defend your position.  This defense should appeal to statistical and philosophical measures of fairness as well as one of our original moral frameworks from the beginning of the course.  Your response should be no more than a paragraph in length. 

The use of COMPAS to supplement a judge's discretion is not justifiable.  The Northpointe defense was mostly based upon the impossibility theorem in which you cannot satisfy all three statistical measures of fairness unless the fringe cases are met, citing that since sufficiency was met they could still be considered justifiable.  However, since disparate impact and equalized odds were both violated, indicating that not only was there discrimination in the algorithm but the discrimination was not in line with a ground truth, the algorithm is not justifiable.  Furthermore, COMPAS was intended to be able to reduce a judge's workload and supplement their decisions, however, most judges lack the statistical training to be able to understand the implications of the algorithm they're using.  For instance, there is a caveat specifying that the model may experience overfitting, which unless you have statistical training you wouldn't be able to fully grasp.  There is also the point that although it is intended to "supplement" a judge's decision they may in fact use it to fully make their decisions in the interest of time.  In terms of philosophical ramifications, under consequentialism, the consequences of using COMPAS far exceed any benefits you may obtain from it.  The overall accuracy of just over 60% is far lower than you'd want to make a decision about recidivism where the consequences of getting it wrong are so dire.  Although the intentions behind using COMPAS may be good, the outcomes are too often wrong, and under the consequentialism framework the cons of using COMPAS far outweigh the pros.

